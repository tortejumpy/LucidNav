# ğŸ‘ï¸â€ğŸ—¨ï¸ LucidNav: Agentic AI Vision Assistant

> **"See through sound. Navigate with intelligence."**
> LucidNav is an agentic, multimodal AI-powered vision assistant designed to help visually impaired users understand and navigate their surroundings in real time using intelligent audio feedback.

---

## ğŸ” Project Overview

LucidNav transforms live camera input into **context-aware spoken guidance** by combining computer vision, gesture recognition, facial emotion analysis, optical character recognition (OCR), and safety mechanisms. Unlike traditional object detection systems, LucidNav behaves as an **agentic AI system** â€” it perceives, reasons, prioritizes, and communicates meaningfully.

The system is designed to work on **consumer-grade hardware**, making it accessible, low-cost, and practical for real-world use.

---

## âœ¨ Key Features

* ğŸ¯ **Real-Time Object Detection** using YOLOv8
* ğŸ“ **Distance & Direction Estimation** (left / right / ahead)
* âœ‹ **Hand Gesture Recognition** (pointing, thumbs up, open hand, victory)
* ğŸ˜ **Facial Emotion Analysis** for detected persons
* ğŸª§ **Live OCR** to read signboards, labels, and printed text
* ğŸ—£ï¸ **Natural Audio Narration** with context-aware summaries
* ğŸ§  **Agentic Reasoning Layer** to fuse multi-modal signals
* ğŸ†˜ **Emergency SOS System** with geolocation and SMS alerts
* âš¡ **Asynchronous & Optimized Pipeline** for smooth real-time performance

---

## ğŸ§  How LucidNav Works (System Architecture)

1. **Vision Perception**

   * Camera feed processed in real time using OpenCV
   * YOLOv8 detects objects with confidence filtering

2. **Spatial Awareness**

   * Distance estimated using focal length and known object widths
   * Direction inferred based on object position in frame

3. **Human Interaction Understanding**

   * MediaPipe detects hand landmarks and gestures
   * DeepFace analyzes facial expressions to infer emotions

4. **Text Understanding**

   * EasyOCR extracts visible text from live frames

5. **Agentic Reasoning**

   * All signals (objects, distance, gestures, emotions, text) are fused
   * The system prioritizes relevance and safety before narration

6. **Audio Feedback & Safety**

   * Context-aware descriptions delivered via Text-to-Speech
   * Double-tap gesture triggers emergency SOS with location sharing

---

## ğŸ§° Tech Stack

| Category            | Technologies                 |
| ------------------- | ---------------------------- |
| Language            | Python 3.9+                  |
| Computer Vision     | OpenCV, YOLOv8 (Ultralytics) |
| Gesture Recognition | MediaPipe Hands              |
| Emotion Analysis    | DeepFace                     |
| OCR                 | EasyOCR                      |
| Speech              | pyttsx3 / plyer.tts          |
| Safety & SOS        | Twilio API, Geocoder         |
| UI                  | Tkinter / Kivy               |
| Concurrency         | Multithreading, Queues       |

---

## ğŸ“ Project Structure

```
LucidNav/
â”‚â”€â”€ main.py                  # Main application entry point
â”‚â”€â”€ android_main.py          # Kivy-based Android-ready version
â”‚â”€â”€ vision_assistant.py      # Core agentic logic (perception + reasoning)
â”‚â”€â”€ float_yolo.py            # YOLO utilities
â”‚â”€â”€ obj.py                   # Object configuration & helpers
â”‚â”€â”€ ui.kv                    # Kivy UI layout
â”‚â”€â”€ yolov8n.pt               # YOLOv8 model weights
â”‚â”€â”€ README.md                # Project documentation
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/LucidNav.git
cd LucidNav
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

> âš ï¸ Note: Some features (DeepFace, EasyOCR) are optional and will gracefully disable if not installed.

### 3ï¸âƒ£ Run the Application

```bash
python main.py
```

---

## ğŸš¨ Emergency SOS Feature

* Triggered via **double-tap / double-click gesture**
* Automatically fetches user location
* Sends SOS SMS with Google Maps link to a predefined contact
* Provides spoken confirmation to the user

---

## ğŸ¯ Use Cases

* Navigation assistance for visually impaired users
* Indoor & outdoor environment understanding
* Reading signboards and printed instructions
* Safety monitoring and emergency alerts
* Human interaction awareness (gestures & emotions)

---

## ğŸš€ Future Enhancements

* ğŸ“± Mobile deployment (Android / iOS)
* ğŸ•¶ï¸ Smart glasses or wearable integration
* ğŸŒ Offline-first edge AI optimization
* ğŸ§­ Path planning and obstacle avoidance
* ğŸ—ºï¸ Indoor mapping and memory-based navigation

---

## ğŸ† Why LucidNav Stands Out

* Goes beyond object detection into **agentic AI reasoning**
* Multimodal perception fused into meaningful narration
* Designed for **real-world accessibility and safety**
* Modular, scalable, and production-oriented architecture

---

## ğŸ‘¤ Author

**Harsh Pandey**
Aspiring AI / ML Engineer | Computer Vision & Agentic AI Enthusiast

---

## ğŸ“œ License

This project is licensed under the MIT License. Feel free to use, modify, and build upon it.

---

â­ *If you found this project useful or inspiring, please consider giving it a star!*
