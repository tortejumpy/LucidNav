# ğŸ‘ï¸â€ğŸ—¨ï¸ Chal-Chitra: Agentic AI-Based Vision Assistant for the Blind

> "See the unseen, navigate the unknown."  
> A powerful agentic AI application that empowers visually impaired individuals to understand their surroundings using real-time computer vision, gesture recognition, OCR, and natural narration.

---

## ğŸ” Project Overview

ChalChitra is an AI-powered assistive tool designed to interpret visual scenes for blind or visually impaired individuals. It provides auditory feedback for surrounding objects, text on signboards, human gestures, and emotional cues â€” all in real time.

Whether navigating indoors or outdoors, LucidNav acts as a virtual guide capable of:
- Recognizing gestures (like â€œthumbs upâ€, â€œpeaceâ€, etc.)
- Identifying objects with estimated distances and directions
- Reading live text (e.g., signs, warnings, door labels)
- Detecting human emotions
- Performing SOS alerts on double-tap gestures

---

## ğŸ§  Core Features

- ğŸ¯ Real-Time Object Detection (YOLOv8)
- âœ‹ Gesture Recognition (MediaPipe Hands)
- ğŸ˜ Facial Emotion Analysis (DeepFace)
- ğŸª§ Live OCR from Camera Feed (EasyOCR)
- ğŸ—£ï¸ Natural Speech Feedback (pyttsx3)
- ğŸ†˜ Emergency SOS + Geolocation Sharing (Twilio + Geocoder)
- ğŸ–±ï¸ Double-Click Trigger for Safety Alerts
- ğŸ”Š Works offline for most features (speech, vision, gesture)

---

## ğŸ§° Tech Stack

| Layer | Tools/Frameworks |
|------|------------------|
| ğŸ‘ï¸ Vision | OpenCV, MediaPipe, YOLOv8 |
| ğŸ’¬ Speech | pyttsx3 |
| ğŸ“– OCR | EasyOCR |
| ğŸ˜Š Emotion | DeepFace |
| ğŸŒ Location/SOS | Geocoder, Twilio API |
| ğŸ§  Agentic AI | Threaded logic + context fusion |
| ğŸ›ï¸ GUI | Tkinter, PIL |
| ğŸ Language | Python 3.9 |

---

## ğŸ–¥ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/LucidNav.git
cd LucidNav/backend
